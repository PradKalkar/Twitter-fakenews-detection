{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2022-07-31T06:50:57.036380Z","iopub.execute_input":"2022-07-31T06:50:57.037273Z","iopub.status.idle":"2022-07-31T06:51:09.626493Z","shell.execute_reply.started":"2022-07-31T06:50:57.037161Z","shell.execute_reply":"2022-07-31T06:51:09.625519Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.18.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.64.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.11.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.12.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.96)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.5.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.2.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.11.4)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.53)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.11.10)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (1.1.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.1.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.5.18.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.9)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (8.0.4)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=aa5de0a8de7bd16942dfe4bfe94ee6c35412cf0f4ace20c4d6fc206c50da2561\n  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom sentence_transformers import util\nfrom datasets import load_metric\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-07-31T06:51:11.129100Z","iopub.execute_input":"2022-07-31T06:51:11.129878Z","iopub.status.idle":"2022-07-31T06:51:18.943773Z","shell.execute_reply.started":"2022-07-31T06:51:11.129842Z","shell.execute_reply":"2022-07-31T06:51:18.943000Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"task = \"rte\" # recognising textual entailment (1 among the 9 GLUE tasks)\nmetric = load_metric(\"glue\", task) # This will be accuracy","metadata":{"execution":{"iopub.status.busy":"2022-07-31T06:51:30.240335Z","iopub.execute_input":"2022-07-31T06:51:30.240912Z","iopub.status.idle":"2022-07-31T06:51:30.534746Z","shell.execute_reply.started":"2022-07-31T06:51:30.240883Z","shell.execute_reply":"2022-07-31T06:51:30.534020Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3389f5da525943db82e4ad51378b5cb4"}},"metadata":{}}]},{"cell_type":"code","source":"# This function is to be fed into the HuggingFace Trainer API to compute the accuracy\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T06:51:57.540832Z","iopub.execute_input":"2022-07-31T06:51:57.541179Z","iopub.status.idle":"2022-07-31T06:51:57.546188Z","shell.execute_reply.started":"2022-07-31T06:51:57.541151Z","shell.execute_reply":"2022-07-31T06:51:57.545276Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# pytorch dataset class for recognising textual entailment\nclass RTE_Dataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T06:51:59.258455Z","iopub.execute_input":"2022-07-31T06:51:59.259024Z","iopub.status.idle":"2022-07-31T06:51:59.265352Z","shell.execute_reply.started":"2022-07-31T06:51:59.258988Z","shell.execute_reply":"2022-07-31T06:51:59.264284Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Thus us basically a pretrained \"distilbert-base-uncased\" finetuned for our dataset\ndef entailment_model(train_dataset, validation_dataset):\n  print(train_dataset.shape, validation_dataset.shape)\n  model_checkpoint = \"distilbert-base-uncased\"\n  train_batch_size = 8 # Hyperparameter (can be tuned)\n  val_batch_size = 1 # Hyperparameter (can be tuned)\n\n  tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n  train_encodings = tokenizer(train_dataset[\"Tweet(Hypothesis)\"].to_list(), train_dataset[\"RelevantSentences(Premise)\"].tolist(), padding=True, truncation=True)\n  validation_encodings = tokenizer(validation_dataset[\"Tweet(Hypothesis)\"].to_list(), validation_dataset[\"RelevantSentences(Premise)\"].tolist(), padding=True, truncation=True)\n\n  label_dict = {\"fake\": 0, \"real\": 1}\n  train_labels = train_dataset[\"Label\"].map(label_dict).to_list()\n  validation_labels = validation_dataset[\"Label\"].map(label_dict).to_list()\n\n  # creating the pytorch training and validation datasets from the tokenized encodings\n  train_dataset_torch = RTE_Dataset(train_encodings, train_labels)\n  validation_dataset_torch = RTE_Dataset(validation_encodings, validation_labels)\n\n  # initialising the model and adding one output neural layer for classification\n  num_labels = 2\n  model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n\n  # training arguments to customize the training\n  # consists some hyperparameters like weight decay, epochs\n  training_args = TrainingArguments(\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    output_dir='./results',          # output directory\n    num_train_epochs=10,              # total number of training epochs\n    per_device_train_batch_size=train_batch_size,  # batch size per device during training\n    per_device_eval_batch_size=val_batch_size,   # batch size for evaluation\n    weight_decay=0.01,               # strength of weight decay\n    metric_for_best_model=\"accuracy\",\n    load_best_model_at_end=True\n  )\n\n  # prints the deviceee - cuda or cpu\n  print(\"Training device:\", training_args.device)\n\n  # using the Trainer API to specify training\n  trainer = Trainer(\n    model=model,                         # the instantiated ü§ó Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset_torch,         # training dataset\n    eval_dataset=validation_dataset_torch,     # evaluation dataset\n    compute_metrics=compute_metrics\n  )\n\n  # training the model\n  trainer.train()\n\n  # using the trained model to return the predictions object and extracting accuracy from it\n  predictions = trainer.predict(test_dataset=validation_dataset_torch)\n  print(\"Accuracy after finetuning:\", predictions.metrics['test_accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-07-31T06:52:01.167520Z","iopub.execute_input":"2022-07-31T06:52:01.167866Z","iopub.status.idle":"2022-07-31T06:52:01.179260Z","shell.execute_reply.started":"2022-07-31T06:52:01.167837Z","shell.execute_reply":"2022-07-31T06:52:01.178462Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# This uses the 'stsb-mpnet-base-v2' pretrained model but it is not fine-tuned on our dataset\n# generates embeddings and classifies entailmenr based on threshold\ndef entailment_without_finetuning(model, tweet, evidence_set, threshold):\n    tweet_embedding = model.encode(tweet, convert_to_tensor=True)\n    evidence_embedding = model.encode(evidence_set, convert_to_tensor = True)\n    cosine_score = util.pytorch_cos_sim(tweet_embedding, evidence_embedding)\n    similarity_score = cosine_score.item()\n    if similarity_score > threshold:\n        return \"real\"\n    else:\n        return \"fake\"","metadata":{"execution":{"iopub.status.busy":"2022-07-31T06:52:01.180709Z","iopub.execute_input":"2022-07-31T06:52:01.181467Z","iopub.status.idle":"2022-07-31T06:52:01.194257Z","shell.execute_reply.started":"2022-07-31T06:52:01.181432Z","shell.execute_reply":"2022-07-31T06:52:01.193568Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_df_list = []\nfor i in range(460):\n    train_df_i = pd.read_csv(f'../input/twitter-training-set-entailment/training_dataset_{i}.csv')\n    train_df_list.append(train_df_i)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T06:53:28.351626Z","iopub.execute_input":"2022-07-31T06:53:28.352526Z","iopub.status.idle":"2022-07-31T06:53:30.176867Z","shell.execute_reply.started":"2022-07-31T06:53:28.352491Z","shell.execute_reply":"2022-07-31T06:53:30.176064Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_df = pd.concat(train_df_list, axis=0)\ntrain_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-31T06:53:33.419036Z","iopub.execute_input":"2022-07-31T06:53:33.419406Z","iopub.status.idle":"2022-07-31T06:53:33.740760Z","shell.execute_reply.started":"2022-07-31T06:53:33.419378Z","shell.execute_reply":"2022-07-31T06:53:33.740016Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(9200, 5)"},"metadata":{}}]},{"cell_type":"code","source":"print('Starting to train (fine-tune distilbert-base-uncased)...')\ntrain_df = train_df.sample(frac=1).reset_index() # shuffle the dataset\n# train_df.dropna(inplace=True)\ntrain_df['Tweet(Hypothesis)'] = train_df['Tweet(Hypothesis)'].fillna(\"\")\ntrain_df['RelevantSentences(Premise)'] = train_df['RelevantSentences(Premise)'].fillna(\"\")","metadata":{"execution":{"iopub.status.busy":"2022-07-31T06:53:39.693401Z","iopub.execute_input":"2022-07-31T06:53:39.693957Z","iopub.status.idle":"2022-07-31T06:53:39.719697Z","shell.execute_reply.started":"2022-07-31T06:53:39.693916Z","shell.execute_reply":"2022-07-31T06:53:39.718880Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Starting to train (fine-tune distilbert-base-uncased)...\n","output_type":"stream"}]},{"cell_type":"code","source":"print('train_df shape before train_test_split:', train_df.shape)\ntrain_dataset, validation_dataset = train_test_split(train_df, test_size=0.25)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T06:55:16.894625Z","iopub.execute_input":"2022-07-31T06:55:16.894985Z","iopub.status.idle":"2022-07-31T06:55:16.903981Z","shell.execute_reply.started":"2022-07-31T06:55:16.894958Z","shell.execute_reply":"2022-07-31T06:55:16.903163Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"train_df shape before train_test_split: (9200, 6)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df['Tweet(Hypothesis)'].isna().sum(), train_df['RelevantSentences(Premise)'].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-31T06:55:37.834148Z","iopub.execute_input":"2022-07-31T06:55:37.834740Z","iopub.status.idle":"2022-07-31T06:55:37.843517Z","shell.execute_reply.started":"2022-07-31T06:55:37.834697Z","shell.execute_reply":"2022-07-31T06:55:37.842746Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(0, 0)"},"metadata":{}}]},{"cell_type":"code","source":"entailment_model(train_dataset, validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T06:55:39.154710Z","iopub.execute_input":"2022-07-31T06:55:39.155051Z","iopub.status.idle":"2022-07-31T07:33:02.424058Z","shell.execute_reply.started":"2022-07-31T06:55:39.155023Z","shell.execute_reply":"2022-07-31T07:33:02.422986Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.18.0\",\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"},{"name":"stdout","text":"(6900, 6) (2300, 6)\n","output_type":"stream"},{"name":"stderr","text":"loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nloading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\nloading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\nloading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.18.0\",\n  \"vocab_size\": 30522\n}\n\nloading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.18.0\",\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 6900\n  Num Epochs = 10\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 8630\n","output_type":"stream"},{"name":"stdout","text":"Training device: cuda:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8630' max='8630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8630/8630 36:56, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.326900</td>\n      <td>0.293721</td>\n      <td>0.898261</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.219200</td>\n      <td>0.240270</td>\n      <td>0.900435</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.196900</td>\n      <td>0.310303</td>\n      <td>0.903478</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.162400</td>\n      <td>0.272088</td>\n      <td>0.910870</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.146000</td>\n      <td>0.353338</td>\n      <td>0.903478</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.141800</td>\n      <td>0.364585</td>\n      <td>0.905652</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.127500</td>\n      <td>0.392493</td>\n      <td>0.900435</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.100200</td>\n      <td>0.459173</td>\n      <td>0.906957</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.084200</td>\n      <td>0.463725</td>\n      <td>0.907826</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.077400</td>\n      <td>0.503498</td>\n      <td>0.906087</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 2300\n  Batch size = 1\nSaving model checkpoint to ./results/checkpoint-863\nConfiguration saved in ./results/checkpoint-863/config.json\nModel weights saved in ./results/checkpoint-863/pytorch_model.bin\n***** Running Evaluation *****\n  Num examples = 2300\n  Batch size = 1\nSaving model checkpoint to ./results/checkpoint-1726\nConfiguration saved in ./results/checkpoint-1726/config.json\nModel weights saved in ./results/checkpoint-1726/pytorch_model.bin\n***** Running Evaluation *****\n  Num examples = 2300\n  Batch size = 1\nSaving model checkpoint to ./results/checkpoint-2589\nConfiguration saved in ./results/checkpoint-2589/config.json\nModel weights saved in ./results/checkpoint-2589/pytorch_model.bin\n***** Running Evaluation *****\n  Num examples = 2300\n  Batch size = 1\nSaving model checkpoint to ./results/checkpoint-3452\nConfiguration saved in ./results/checkpoint-3452/config.json\nModel weights saved in ./results/checkpoint-3452/pytorch_model.bin\n***** Running Evaluation *****\n  Num examples = 2300\n  Batch size = 1\nSaving model checkpoint to ./results/checkpoint-4315\nConfiguration saved in ./results/checkpoint-4315/config.json\nModel weights saved in ./results/checkpoint-4315/pytorch_model.bin\n***** Running Evaluation *****\n  Num examples = 2300\n  Batch size = 1\nSaving model checkpoint to ./results/checkpoint-5178\nConfiguration saved in ./results/checkpoint-5178/config.json\nModel weights saved in ./results/checkpoint-5178/pytorch_model.bin\n***** Running Evaluation *****\n  Num examples = 2300\n  Batch size = 1\nSaving model checkpoint to ./results/checkpoint-6041\nConfiguration saved in ./results/checkpoint-6041/config.json\nModel weights saved in ./results/checkpoint-6041/pytorch_model.bin\n***** Running Evaluation *****\n  Num examples = 2300\n  Batch size = 1\nSaving model checkpoint to ./results/checkpoint-6904\nConfiguration saved in ./results/checkpoint-6904/config.json\nModel weights saved in ./results/checkpoint-6904/pytorch_model.bin\n***** Running Evaluation *****\n  Num examples = 2300\n  Batch size = 1\nSaving model checkpoint to ./results/checkpoint-7767\nConfiguration saved in ./results/checkpoint-7767/config.json\nModel weights saved in ./results/checkpoint-7767/pytorch_model.bin\n***** Running Evaluation *****\n  Num examples = 2300\n  Batch size = 1\nSaving model checkpoint to ./results/checkpoint-8630\nConfiguration saved in ./results/checkpoint-8630/config.json\nModel weights saved in ./results/checkpoint-8630/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3452 (score: 0.9108695652173913).\n***** Running Prediction *****\n  Num examples = 2300\n  Batch size = 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2300' max='2300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2300/2300 00:21]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Accuracy after finetuning: 0.9108695652173913\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}